{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 00:47:05.109830: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-15 00:47:07.019206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 00:47:07.019898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 00:47:07.318405: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 00:47:08.353723: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 00:47:16.999160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Sequential, optimizers, layers, Model\n",
    "from keras.models import load_model\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression, SGDOneClassSVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix, balanced_accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from npz file, this is already broken down into training validation and testing datasets, put these into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11959, 28, 28, 3)\n",
      "(1712, 28, 28, 3)\n",
      "(3421, 28, 28, 3)\n",
      "(11959, 1)\n",
      "(1712, 1)\n",
      "(3421, 1)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"bloodmnist.npz\")\n",
    "train_images = data[\"train_images\"]\n",
    "print(np.shape(data[\"train_images\"]))\n",
    "val_images = data[\"val_images\"]\n",
    "print(np.shape(data[\"val_images\"]))\n",
    "test_images = data[\"test_images\"]\n",
    "print(np.shape(data[\"test_images\"]))\n",
    "train_labels = data[\"train_labels\"]\n",
    "print(np.shape(data[\"train_labels\"]))\n",
    "val_labels = data[\"val_labels\"]\n",
    "print(np.shape(data[\"val_labels\"]))\n",
    "test_labels = data[\"test_labels\"]\n",
    "print(np.shape(data[\"test_labels\"]))\n",
    "\n",
    "combine_train_images = np.append(train_images, val_images, axis=0)\n",
    "combine_train_labels = np.append(train_labels, val_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        self.length = np.shape(images)[0]\n",
    "        self.width = np.shape(images)[1]\n",
    "        # self.iamge_features = []\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "        # self.oversample()\n",
    "\n",
    "    def update_counts(self):\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            self.counts.append(len(np.where(self.labels == i)[0]))\n",
    "        \n",
    "        self.proportions = [count/self.length for count in self.counts]\n",
    "\n",
    "    def oversample(self):\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        self.images= self.images.reshape((self.length, self.width*self.width*3))\n",
    "        self.images, self.labels = ros.fit_resample(self.images, self.labels)\n",
    "        self.length = self.images.shape[0]\n",
    "        self.images = self.images.reshape((self.length, self.width, self.width, 3))\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "        self.image_features = self.images\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        one_hot_labels = np.array([np.zeros(self.class_num) for i in range(self.length)])\n",
    "        for i in range(self.length):\n",
    "            one_hot_labels[i][self.labels[i]] = 1\n",
    "        return np.argmax(one_hot_labels, axis=1)\n",
    "    \n",
    "    # def apply_CNN(self, model):\n",
    "\n",
    "    def shuffle(self):\n",
    "        p = np.random.permutation(self.length)\n",
    "        self.images, self.labels, self.one_hot_labels = self.images[p], self.labels[p], self.one_hot_labels[p]\n",
    "\n",
    "    def get_features(self, model):\n",
    "        print(np.shape(self.images), np.shape(self.one_hot_labels))\n",
    "        self.image_features = model.predict(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = load_model(\"./CNN_model.h5\")\n",
    "model_use = Model(\n",
    "    inputs = cnn_model.input,\n",
    "    outputs = cnn_model.layers[-3].output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_images, train_labels)\n",
    "val_dataset = ImageDataset(val_images, val_labels)\n",
    "combine_train_dataset = ImageDataset(combine_train_images, combine_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18640, 28, 28, 3) (18640,)\n",
      "583/583 [==============================] - 13s 13ms/step\n",
      "(1712, 28, 28, 3) (1712,)\n",
      "54/54 [==============================] - 1s 10ms/step\n",
      "(13671, 28, 28, 3) (13671,)\n",
      "428/428 [==============================] - 5s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "train_dataset.oversample()\n",
    "train_dataset.get_features(model_use)\n",
    "val_dataset.get_features(model_use)\n",
    "combine_train_dataset.get_features(model_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-dv/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;sag&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;sag&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='sag')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define dataset\n",
    "# bcl = LogisticRegression(max_iter=1000)\n",
    "# model = DecisionTreeClassifier(splitter='random')\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestClassifier(max_depth=10, n_estimators=50)),\n",
    "#     # ('lr', LogisticRegression(C=0.5, tol=0.001, solver=\"sag\", max_iter=10000)),\n",
    "#     ('lr', LogisticRegression(max_iter=10000, C=0.5))\n",
    "#     # ('knn', KNeighborsClassifier())\n",
    "# ]\n",
    "# bcl = RandomForestClassifier(max_depth=10)\n",
    "# model = GaussianNB()\n",
    "# model = KNeighborsClassifier()\n",
    "# model = SGDOneClassSVM()\n",
    "# fit model\n",
    "# bcl = HistGradientBoostingClassifier(random_state=0, max_depth=2, max_iter=20, learning_rate=0.1, scoring='loss', l2_regularization=0.1, max_leaf_nodes=20)\n",
    "# bcl = AdaBoostClassifier()\n",
    "param_grid = {\n",
    "    'classification__learning_rate': [0.01, 0.1, 1],\n",
    "    'classification__n_estimators': [10, 20, 50],\n",
    "    'classification__estimator__max_depth': [2,5,10],\n",
    "    'classification__estimator__min_samples_split': [5, 10, 20],\n",
    "    'classification__estimator__min_samples_leaf': [5]\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'balanced_accuracy_score': make_scorer(balanced_accuracy_score)\n",
    "}\n",
    "\n",
    "model = Pipeline([\n",
    "    ('sampling', RandomOverSampler()),\n",
    "    ('classification', AdaBoostClassifier(DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "# bcl = GridSearchCV(estimator=model, param_grid=param_grid, scoring='balanced_accuracy', cv=5, refit=True)\n",
    "# bcl = BaggingClassifier(DecisionTreeClassifier(min_samples_leaf=20, max_depth=3, min_samples_split=50), n_estimators=100)\n",
    "bcl = LogisticRegression(solver='sag', max_iter=1000)\n",
    "bcl.fit(train_dataset.image_features, train_dataset.one_hot_labels)\n",
    "# print(bcl.best_params_)\n",
    "# make predictions\n",
    "# results_df = pd.DataFrame(bcl.results)\n",
    "# results_df.to_csv('results.csv', encoding='utf-8', index=False)\n",
    "# yhat = bcl.predict(train_dataset.image_features)\n",
    "# print(yhat)\n",
    "# models = []\n",
    "# for i in range(train_dataset.class_num * train_dataset.class_num-1):\n",
    "#     models.add(Dense(100, activation='relu'))\n",
    "#     models.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746244635193133\n"
     ]
    }
   ],
   "source": [
    "# get accuracy and auc\n",
    "# results_df = pd.DataFrame(bcl.results)\n",
    "# results_df.to_csv('results.csv', encoding='utf-8', index=False)\n",
    "# yhat = bcl.predict(combine_train_dataset.image_features)\n",
    "yhat = bcl.predict(train_dataset.image_features)\n",
    "\n",
    "acc = metrics.accuracy_score(train_dataset.labels, yhat)\n",
    "# metrics.auc(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3421, 28, 28, 3) (3421,)\n",
      "107/107 [==============================] - 1s 11ms/step\n",
      "0.9193218357205496\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = ImageDataset(test_images, test_labels)\n",
    "# test_dataset.get_features(model_use)\n",
    "valhat = bcl.predict(val_dataset.image_features)\n",
    "acc = metrics.accuracy_score(val_dataset.one_hot_labels, valhat)\n",
    "# print(search.best_params_)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AdaBoostClassifier' object has no attribute 'train_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m \u001b[43mbcl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_score_\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(train_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), train_scores, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AdaBoostClassifier' object has no attribute 'train_score_'"
     ]
    }
   ],
   "source": [
    "train_scores = bcl.train_score_\n",
    "plt.plot(np.arange(train_scores.shape[0]), train_scores, 'b-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
