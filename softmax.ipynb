{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 23:03:26.119149: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-13 23:03:26.644342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-13 23:03:26.644900: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-13 23:03:26.734445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-13 23:03:27.001601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-13 23:03:29.754155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, AveragePooling2D, Input\n",
    "from keras import Sequential, optimizers, layers\n",
    "from keras.models import load_model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import keras_tuner as kt\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from npz file,\n",
    ".npz is used to save numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11959, 28, 28, 3)\n",
      "(1712, 28, 28, 3)\n",
      "(3421, 28, 28, 3)\n",
      "(11959, 1)\n",
      "(1712, 1)\n",
      "(3421, 1)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"bloodmnist.npz\")\n",
    "train_images = data[\"train_images\"]\n",
    "print(np.shape(data[\"train_images\"]))\n",
    "val_images = data[\"val_images\"]\n",
    "print(np.shape(data[\"val_images\"]))\n",
    "test_images = data[\"test_images\"]\n",
    "print(np.shape(data[\"test_images\"]))\n",
    "train_labels = data[\"train_labels\"]\n",
    "print(np.shape(data[\"train_labels\"]))\n",
    "val_labels = data[\"val_labels\"]\n",
    "print(np.shape(data[\"val_labels\"]))\n",
    "test_labels = data[\"test_labels\"]\n",
    "print(np.shape(data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        self.length = np.shape(images)[0]\n",
    "        self.width = np.shape(images)[1]\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    def update_counts(self):\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            self.counts.append(len(np.where(self.labels == i)[0]))\n",
    "        \n",
    "        self.proportions = [count/self.length for count in self.counts]\n",
    "\n",
    "    def oversample(self):\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        self.images= self.images.reshape((self.length, self.width*self.width*3))\n",
    "        self.images, self.labels = ros.fit_resample(self.images, self.labels)\n",
    "        self.length = self.images.shape[0]\n",
    "        print(self.length)\n",
    "        print(np.shape(self.images))\n",
    "        print(self.labels.shape)\n",
    "        self.images = self.images.reshape((self.length, self.width, self.width, 3))\n",
    "        # print(self.images[0])\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        one_hot_labels = np.array([np.zeros(self.class_num) for i in range(self.length)])\n",
    "        for i in range(self.length):\n",
    "            one_hot_labels[i][self.labels[i]] = 1\n",
    "        return one_hot_labels\n",
    "    \n",
    "    # def apply_CNN(self, model):\n",
    "\n",
    "    def shuffle(self):\n",
    "        p = np.random.permutation(self.length)\n",
    "        self.images, self.labels, self.one_hot_labels = self.images[p], self.labels[p], self.one_hot_labels[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise classes for training, validation and testing validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[852, 2181, 1085, 2026, 849, 993, 2330, 1643] [0.07124341500125428, 0.182373108119408, 0.09072664938540012, 0.16941215820720795, 0.07099255790617945, 0.08303369846977172, 0.1948323438414583, 0.1373860690693202]\n",
      "[122, 312, 155, 290, 122, 143, 333, 235] [0.07126168224299065, 0.1822429906542056, 0.0905373831775701, 0.169392523364486, 0.07126168224299065, 0.08352803738317757, 0.19450934579439252, 0.13726635514018692]\n",
      "[244, 624, 311, 579, 243, 284, 666, 470] [0.07132417421806489, 0.1824028061970184, 0.09090909090909091, 0.16924875767319497, 0.0710318620286466, 0.08301666179479684, 0.19467991815258695, 0.1373867290266004]\n"
     ]
    }
   ],
   "source": [
    "# Initialise Class for training, validation, test\n",
    "train_dataset = ImageDataset(train_images, train_labels)\n",
    "val_dataset = ImageDataset(val_images, val_labels)\n",
    "test_dataset = ImageDataset(test_images, test_labels)\n",
    "\n",
    "# print counts and proportions to see if data needs to be balanced\n",
    "print(train_dataset.counts, train_dataset.proportions)\n",
    "print(val_dataset.counts, val_dataset.proportions)\n",
    "print(test_dataset.counts, test_dataset.proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset found to be imbalanced, oversample training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18640\n",
      "(18640, 2352)\n",
      "(18640,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset.oversample()\n",
    "# print(train_dataset.counts)\n",
    "# print(np.shape(train_dataset.images))\n",
    "# print(np.shape(train_dataset.one_hot_labels))\n",
    "# one hot encode labels\n",
    "\n",
    "# def one_hot_encode(label):\n",
    "#     return utils.to_categorical(label, num_classes=train_dataset.class_num)\n",
    "# one_hot_train = np.array([utils.to_categorical(label, num_classes=train_dataset.class_num) for label in train_dataset.labels])\n",
    "# print(np.shape(one_hot_train))\n",
    "# one_hot_train = one_hot_train.reshape(np.shape(one_hot_train)[0], train_dataset.class_num)\n",
    "# print(np.shape(one_hot_train))\n",
    "\n",
    "train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Data Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preprocessing(model):\n",
    "    # model.add(layers.RandomBrightness(factor=0.2))\n",
    "    model.add(layers.RandomFlip(mode=\"horizontal_and_vertical\"))\n",
    "    model.add(layers.RandomZoom(height_factor=0.2))\n",
    "    # model.add(layers.RandomRotation(factor=0.2))\n",
    "    model.add(layers.RandomContrast(factor=0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(BatchNormalization())\n",
    "\n",
    "class CNN(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        # define hyperpamaters search space for tuning\n",
    "        filters_1 = hp.Int('filters_1', min_value=16, max_value=512, step=32)\n",
    "        filters_2 = hp.Int('filters_2', min_value=16, max_value=512, step=32)\n",
    "        filters_3 = hp.Int('filters_3', min_value=16, max_value=512, step=32)\n",
    "        dense_1_size = hp.Int('size_1', min_value=10, max_value=510, step=50)\n",
    "        learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape = (28, 28, 3)))\n",
    "        model.add(Conv2D(filters=filters_1, kernel_size=(3, 3), activation='relu', input_shape = (28, 28, 3), strides=1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        model.add(Conv2D(filters=filters_2, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        model.add(Conv2D(filters = filters_3, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(.1))\n",
    "        model.add(Dense(dense_1_size, activation='relu'))\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['AUC', 'categorical_accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=16,\n",
    "            verbose=0\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search optimal hyperparameters and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from optimal_parameters/train_CNN/tuner0.json\n",
      "\n",
      "Search: Running Trial #5\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "112               |112               |filters_1\n",
      "464               |240               |filters_2\n",
      "144               |208               |filters_3\n",
      "260               |210               |size_1\n",
      "0.0001            |0.01              |learning_rate\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# tuner = kt.RandomSearch(\n",
    "#     CNN(),\n",
    "#     objective='val_categorical_accuracy',\n",
    "#     directory='optimal_parameters',\n",
    "#     project_name='train_CNN'\n",
    "# )\n",
    "\n",
    "# tuner.search(train_dataset.images, train_dataset.one_hot_labels, epochs=11, validation_data=(val_dataset.images, val_dataset.one_hot_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Grid search to optimize algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = (28, 28, 3)))\n",
    "model.add(Conv2D(filters=48, kernel_size=(3, 3), activation='relu', input_shape = (28, 28, 3), strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=240, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters = 208, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['AUC', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "2330/2330 [==============================] - 27s 11ms/step - loss: 1.2789 - auc: 0.8812 - categorical_accuracy: 0.5137 - val_loss: 0.7928 - val_auc: 0.9575 - val_categorical_accuracy: 0.7044\n",
      "Epoch 2/11\n",
      "2330/2330 [==============================] - 26s 11ms/step - loss: 0.8293 - auc: 0.9504 - categorical_accuracy: 0.6821 - val_loss: 0.7785 - val_auc: 0.9562 - val_categorical_accuracy: 0.6840\n",
      "Epoch 3/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.6561 - auc: 0.9688 - categorical_accuracy: 0.7503 - val_loss: 0.5027 - val_auc: 0.9809 - val_categorical_accuracy: 0.8137\n",
      "Epoch 4/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.5400 - auc: 0.9786 - categorical_accuracy: 0.7969 - val_loss: 0.4063 - val_auc: 0.9873 - val_categorical_accuracy: 0.8446\n",
      "Epoch 5/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.4621 - auc: 0.9840 - categorical_accuracy: 0.8260 - val_loss: 0.3909 - val_auc: 0.9882 - val_categorical_accuracy: 0.8551\n",
      "Epoch 6/11\n",
      "2330/2330 [==============================] - 26s 11ms/step - loss: 0.3984 - auc: 0.9877 - categorical_accuracy: 0.8512 - val_loss: 0.3566 - val_auc: 0.9897 - val_categorical_accuracy: 0.8668\n",
      "Epoch 7/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.3446 - auc: 0.9907 - categorical_accuracy: 0.8742 - val_loss: 0.2913 - val_auc: 0.9920 - val_categorical_accuracy: 0.8949\n",
      "Epoch 8/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.3002 - auc: 0.9927 - categorical_accuracy: 0.8896 - val_loss: 0.3100 - val_auc: 0.9909 - val_categorical_accuracy: 0.8861\n",
      "Epoch 9/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.2601 - auc: 0.9942 - categorical_accuracy: 0.9035 - val_loss: 0.2630 - val_auc: 0.9931 - val_categorical_accuracy: 0.9089\n",
      "Epoch 10/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.2352 - auc: 0.9952 - categorical_accuracy: 0.9144 - val_loss: 0.2474 - val_auc: 0.9939 - val_categorical_accuracy: 0.9089\n",
      "Epoch 11/11\n",
      "2330/2330 [==============================] - 25s 11ms/step - loss: 0.2116 - auc: 0.9961 - categorical_accuracy: 0.9208 - val_loss: 0.2621 - val_auc: 0.9926 - val_categorical_accuracy: 0.9118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f97c89ef8e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset.images, train_dataset.one_hot_labels, validation_data=(val_dataset.images, val_dataset.one_hot_labels), batch_size=8, epochs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 9ms/step - loss: 0.2621 - auc: 0.9926 - categorical_accuracy: 0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-dv/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(val_dataset.images, val_dataset.one_hot_labels)\n",
    "model.save(\"./CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVRDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.splits = self.create_splits()\n",
    "\n",
    "    def create_splits(self):\n",
    "        splits = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            binary_labels = np.array([1 if label == i else 0 for label in self.labels])\n",
    "            splits.append(ImageDataset(self.images.copy(), binary_labels))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def oversample_dataset(self):\n",
    "        for split in self.splits:\n",
    "            split.oversample()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_' is not defined"
     ]
    }
   ],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # define dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# # define model\n",
    "# model = LogisticRegression(multi_class='ovr')\n",
    "# # fit model\n",
    "# model.fit(X, y)\n",
    "# # make predictions\n",
    "# yhat = model.predict(X)\n",
    "\n",
    "# trainOVR.oversample_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
