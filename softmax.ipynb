{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 12:28:06.063958: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 12:28:07.761864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 12:28:07.763069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 12:28:07.885943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 12:28:08.251479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 12:28:11.982329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, AveragePooling2D, Input\n",
    "from keras import layers\n",
    "from keras import Sequential, optimizers, layers\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow.random as random_tf\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from npz file,\n",
    ".npz is used to save numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11959, 28, 28, 3)\n",
      "(1712, 28, 28, 3)\n",
      "(3421, 28, 28, 3)\n",
      "(11959, 1)\n",
      "(1712, 1)\n",
      "(3421, 1)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"bloodmnist.npz\")\n",
    "train_images = data[\"train_images\"]\n",
    "print(np.shape(data[\"train_images\"]))\n",
    "val_images = data[\"val_images\"]\n",
    "print(np.shape(data[\"val_images\"]))\n",
    "test_images = data[\"test_images\"]\n",
    "print(np.shape(data[\"test_images\"]))\n",
    "train_labels = data[\"train_labels\"]\n",
    "print(np.shape(data[\"train_labels\"]))\n",
    "val_labels = data[\"val_labels\"]\n",
    "print(np.shape(data[\"val_labels\"]))\n",
    "test_labels = data[\"test_labels\"]\n",
    "print(np.shape(data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        self.length = np.shape(images)[0]\n",
    "        self.width = np.shape(images)[1]\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    def update_counts(self):\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            self.counts.append(len(np.where(self.labels == i)[0]))\n",
    "        \n",
    "        self.proportions = [count/self.length for count in self.counts]\n",
    "\n",
    "    def oversample(self):\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        self.images= self.images.reshape((self.length, self.width*self.width*3))\n",
    "        self.images, self.labels = ros.fit_resample(self.images, self.labels)\n",
    "        self.length = self.images.shape[0]\n",
    "        print(self.length)\n",
    "        print(np.shape(self.images))\n",
    "        print(self.labels.shape)\n",
    "        self.images = self.images.reshape((self.length, self.width, self.width, 3))\n",
    "        # print(self.images[0])\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        one_hot_labels = np.array([np.zeros(self.class_num) for i in range(self.length)])\n",
    "        for i in range(self.length):\n",
    "            one_hot_labels[i][self.labels[i]] = 1\n",
    "        return one_hot_labels\n",
    "    \n",
    "    # def apply_CNN(self, model):\n",
    "\n",
    "    def shuffle(self):\n",
    "        p = np.random.permutation(self.length)\n",
    "        self.images, self.labels, self.one_hot_labels = self.images[p], self.labels[p], self.one_hot_labels[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise classes for training, validation and testing validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[852, 2181, 1085, 2026, 849, 993, 2330, 1643] [0.07124341500125428, 0.182373108119408, 0.09072664938540012, 0.16941215820720795, 0.07099255790617945, 0.08303369846977172, 0.1948323438414583, 0.1373860690693202]\n",
      "[122, 312, 155, 290, 122, 143, 333, 235] [0.07126168224299065, 0.1822429906542056, 0.0905373831775701, 0.169392523364486, 0.07126168224299065, 0.08352803738317757, 0.19450934579439252, 0.13726635514018692]\n",
      "[244, 624, 311, 579, 243, 284, 666, 470] [0.07132417421806489, 0.1824028061970184, 0.09090909090909091, 0.16924875767319497, 0.0710318620286466, 0.08301666179479684, 0.19467991815258695, 0.1373867290266004]\n"
     ]
    }
   ],
   "source": [
    "# Initialise Class for training, validation, test\n",
    "train_dataset = ImageDataset(train_images, train_labels)\n",
    "val_dataset = ImageDataset(val_images, val_labels)\n",
    "test_dataset = ImageDataset(test_images, test_labels)\n",
    "\n",
    "# print counts and proportions to see if data needs to be balanced\n",
    "print(train_dataset.counts, train_dataset.proportions)\n",
    "print(val_dataset.counts, val_dataset.proportions)\n",
    "print(test_dataset.counts, test_dataset.proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset found to be imbalanced, oversample training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18640\n",
      "(18640, 2352)\n",
      "(18640,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset.oversample()\n",
    "# print(train_dataset.counts)\n",
    "# print(np.shape(train_dataset.images))\n",
    "# print(np.shape(train_dataset.one_hot_labels))\n",
    "# one hot encode labels\n",
    "\n",
    "# def one_hot_encode(label):\n",
    "#     return utils.to_categorical(label, num_classes=train_dataset.class_num)\n",
    "# one_hot_train = np.array([utils.to_categorical(label, num_classes=train_dataset.class_num) for label in train_dataset.labels])\n",
    "# print(np.shape(one_hot_train))\n",
    "# one_hot_train = one_hot_train.reshape(np.shape(one_hot_train)[0], train_dataset.class_num)\n",
    "# print(np.shape(one_hot_train))\n",
    "\n",
    "train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Data Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preprocessing(model):\n",
    "    # model.add(layers.RandomBrightness(factor=0.2))\n",
    "    model.add(layers.RandomFlip(mode=\"horizontal_and_vertical\"))\n",
    "    model.add(layers.RandomZoom(height_factor=0.2))\n",
    "    # model.add(layers.RandomRotation(factor=0.2))\n",
    "    model.add(layers.RandomContrast(factor=0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Input(shape = (28, 28, 3)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape = (28, 28, 3), strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "optimizer = optimizers.SGD(0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['AUC', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 1.7262 - auc: 0.7568 - categorical_accuracy: 0.3305 - val_loss: 1.0247 - val_auc: 0.9303 - val_categorical_accuracy: 0.6332\n",
      "Epoch 2/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 1.0557 - auc: 0.9203 - categorical_accuracy: 0.5929 - val_loss: 0.8492 - val_auc: 0.9482 - val_categorical_accuracy: 0.6764\n",
      "Epoch 3/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.9201 - auc: 0.9391 - categorical_accuracy: 0.6459 - val_loss: 0.7993 - val_auc: 0.9543 - val_categorical_accuracy: 0.6963\n",
      "Epoch 4/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.8504 - auc: 0.9477 - categorical_accuracy: 0.6764 - val_loss: 0.8490 - val_auc: 0.9487 - val_categorical_accuracy: 0.7033\n",
      "Epoch 5/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.7922 - auc: 0.9545 - categorical_accuracy: 0.6980 - val_loss: 0.6396 - val_auc: 0.9697 - val_categorical_accuracy: 0.7664\n",
      "Epoch 6/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.7455 - auc: 0.9596 - categorical_accuracy: 0.7164 - val_loss: 0.6497 - val_auc: 0.9693 - val_categorical_accuracy: 0.7623\n",
      "Epoch 7/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.7050 - auc: 0.9639 - categorical_accuracy: 0.7307 - val_loss: 0.5696 - val_auc: 0.9760 - val_categorical_accuracy: 0.7950\n",
      "Epoch 8/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.6668 - auc: 0.9674 - categorical_accuracy: 0.7514 - val_loss: 0.5903 - val_auc: 0.9744 - val_categorical_accuracy: 0.7821\n",
      "Epoch 9/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.6297 - auc: 0.9709 - categorical_accuracy: 0.7660 - val_loss: 0.5067 - val_auc: 0.9810 - val_categorical_accuracy: 0.8183\n",
      "Epoch 10/30\n",
      "2330/2330 [==============================] - 16s 7ms/step - loss: 0.5941 - auc: 0.9743 - categorical_accuracy: 0.7773 - val_loss: 0.4879 - val_auc: 0.9824 - val_categorical_accuracy: 0.8271\n",
      "Epoch 11/30\n",
      "2330/2330 [==============================] - 17s 7ms/step - loss: 0.5635 - auc: 0.9767 - categorical_accuracy: 0.7859 - val_loss: 0.5200 - val_auc: 0.9785 - val_categorical_accuracy: 0.8178\n",
      "Epoch 12/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.5291 - auc: 0.9792 - categorical_accuracy: 0.8031 - val_loss: 0.4311 - val_auc: 0.9862 - val_categorical_accuracy: 0.8493\n",
      "Epoch 13/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.5030 - auc: 0.9811 - categorical_accuracy: 0.8136 - val_loss: 0.4939 - val_auc: 0.9805 - val_categorical_accuracy: 0.8248\n",
      "Epoch 14/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.4815 - auc: 0.9826 - categorical_accuracy: 0.8178 - val_loss: 0.3937 - val_auc: 0.9866 - val_categorical_accuracy: 0.8592\n",
      "Epoch 15/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.4590 - auc: 0.9841 - categorical_accuracy: 0.8279 - val_loss: 0.3741 - val_auc: 0.9879 - val_categorical_accuracy: 0.8627\n",
      "Epoch 16/30\n",
      "2330/2330 [==============================] - 20s 9ms/step - loss: 0.4415 - auc: 0.9852 - categorical_accuracy: 0.8354 - val_loss: 0.3762 - val_auc: 0.9884 - val_categorical_accuracy: 0.8610\n",
      "Epoch 17/30\n",
      "2330/2330 [==============================] - 20s 9ms/step - loss: 0.4227 - auc: 0.9864 - categorical_accuracy: 0.8411 - val_loss: 0.3714 - val_auc: 0.9882 - val_categorical_accuracy: 0.8697\n",
      "Epoch 18/30\n",
      "2330/2330 [==============================] - 20s 9ms/step - loss: 0.4019 - auc: 0.9875 - categorical_accuracy: 0.8505 - val_loss: 0.3470 - val_auc: 0.9891 - val_categorical_accuracy: 0.8797\n",
      "Epoch 19/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.3910 - auc: 0.9880 - categorical_accuracy: 0.8550 - val_loss: 0.3435 - val_auc: 0.9895 - val_categorical_accuracy: 0.8803\n",
      "Epoch 20/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.3719 - auc: 0.9892 - categorical_accuracy: 0.8626 - val_loss: 0.3907 - val_auc: 0.9863 - val_categorical_accuracy: 0.8563\n",
      "Epoch 21/30\n",
      "2330/2330 [==============================] - 20s 8ms/step - loss: 0.3575 - auc: 0.9897 - categorical_accuracy: 0.8698 - val_loss: 0.3539 - val_auc: 0.9877 - val_categorical_accuracy: 0.8756\n",
      "Epoch 22/30\n",
      "2330/2330 [==============================] - 21s 9ms/step - loss: 0.3457 - auc: 0.9906 - categorical_accuracy: 0.8719 - val_loss: 0.3376 - val_auc: 0.9886 - val_categorical_accuracy: 0.8820\n",
      "Epoch 23/30\n",
      "2330/2330 [==============================] - 21s 9ms/step - loss: 0.3310 - auc: 0.9911 - categorical_accuracy: 0.8784 - val_loss: 0.3304 - val_auc: 0.9886 - val_categorical_accuracy: 0.8873\n",
      "Epoch 24/30\n",
      "2330/2330 [==============================] - 20s 9ms/step - loss: 0.3280 - auc: 0.9912 - categorical_accuracy: 0.8805 - val_loss: 0.3344 - val_auc: 0.9889 - val_categorical_accuracy: 0.8884\n",
      "Epoch 25/30\n",
      "2330/2330 [==============================] - 20s 9ms/step - loss: 0.3233 - auc: 0.9916 - categorical_accuracy: 0.8815 - val_loss: 0.3321 - val_auc: 0.9901 - val_categorical_accuracy: 0.8914\n",
      "Epoch 26/30\n",
      "2330/2330 [==============================] - 20s 8ms/step - loss: 0.3122 - auc: 0.9921 - categorical_accuracy: 0.8863 - val_loss: 0.3090 - val_auc: 0.9906 - val_categorical_accuracy: 0.8884\n",
      "Epoch 27/30\n",
      "2330/2330 [==============================] - 20s 9ms/step - loss: 0.2985 - auc: 0.9926 - categorical_accuracy: 0.8923 - val_loss: 0.3027 - val_auc: 0.9903 - val_categorical_accuracy: 0.8954\n",
      "Epoch 28/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.2973 - auc: 0.9925 - categorical_accuracy: 0.8928 - val_loss: 0.3010 - val_auc: 0.9914 - val_categorical_accuracy: 0.8984\n",
      "Epoch 29/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.2814 - auc: 0.9933 - categorical_accuracy: 0.8970 - val_loss: 0.3285 - val_auc: 0.9883 - val_categorical_accuracy: 0.8972\n",
      "Epoch 30/30\n",
      "2330/2330 [==============================] - 19s 8ms/step - loss: 0.2738 - auc: 0.9935 - categorical_accuracy: 0.9032 - val_loss: 0.3080 - val_auc: 0.9899 - val_categorical_accuracy: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f355cf3dd80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset.images, train_dataset.one_hot_labels, validation_data=(val_dataset.images, val_dataset.one_hot_labels), batch_size=8, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 6ms/step - loss: 0.3080 - auc: 0.9899 - categorical_accuracy: 0.8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-dv/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(val_dataset.images, val_dataset.one_hot_labels)\n",
    "model.save(\"./CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVRDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.splits = self.create_splits()\n",
    "\n",
    "    def create_splits(self):\n",
    "        splits = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            binary_labels = np.array([1 if label == i else 0 for label in self.labels])\n",
    "            splits.append(ImageDataset(self.images.copy(), binary_labels))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def oversample_dataset(self):\n",
    "        for split in self.splits:\n",
    "            split.oversample()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_' is not defined"
     ]
    }
   ],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # define dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# # define model\n",
    "# model = LogisticRegression(multi_class='ovr')\n",
    "# # fit model\n",
    "# model.fit(X, y)\n",
    "# # make predictions\n",
    "# yhat = model.predict(X)\n",
    "\n",
    "# trainOVR.oversample_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
