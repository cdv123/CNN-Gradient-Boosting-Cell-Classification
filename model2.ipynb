{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 01:50:47.126661: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-11 01:50:47.308403: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-11 01:50:48.550875: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-11 01:50:48.550937: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-11 01:50:48.556020: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-11 01:50:49.169434: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-11 01:50:49.176047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 01:50:52.751444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, AveragePooling2D\n",
    "from keras import Sequential\n",
    "from keras import backend as K\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"chestmnist.npz\")\n",
    "train_data = data[\"train_images\"]\n",
    "val_data = data[\"val_images\"]\n",
    "test_data = data[\"test_images\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "val_labels = data[\"val_labels\"]\n",
    "test_labels = data[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(78468, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data))\n",
    "print(np.shape(data[\"train_images\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(labels, class_num):\n",
    "    counts = np.zeros(class_num)\n",
    "    for label in labels:\n",
    "        for i in np.where(label != 0)[0]:\n",
    "            counts[i]+=1\n",
    "    # counts = [i/len(labels) for i in counts]\n",
    "\n",
    "    return counts\n",
    "\n",
    "def get_equal_indicies(labels, class_equal):\n",
    "    indicies = [i for i in range(np.shape(labels)[0]) if labels[i][class_equal] == 1]\n",
    "    return indicies\n",
    "\n",
    "def remove_class(data, labels, class_to_remove):\n",
    "    chosen_indicies = set(get_equal_indicies(labels, class_to_remove))\n",
    "    remaining_indicies = [i for i in range(np.shape(data)[0]) if i not in chosen_indicies]\n",
    "    data, labels = data[remaining_indicies], labels[remaining_indicies]\n",
    "    labels = np.delete(labels, (class_to_remove), axis=1)\n",
    "    return data, labels\n",
    "\n",
    "def add_no_class(labels):\n",
    "    to_append = np.array([np.array([1]) if (label == np.zeros(np.shape(label)[0])).all() else np.array([0]) for label in labels])\n",
    "    labels = np.append(labels, to_append, axis=1)\n",
    "    return labels\n",
    "\n",
    "class_num = np.shape(train_labels)[1]\n",
    "# print(np.shape(train_labels))\n",
    "train_counts = get_counts(train_labels, class_num+1)\n",
    "train_data, train_labels = remove_class(train_data, train_labels, class_num-1)\n",
    "train_counts = get_counts(train_labels, class_num)\n",
    "class_num-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, labels, class_num):\n",
    "    split_labels = []\n",
    "    split_data = []\n",
    "\n",
    "    for i in range(class_num):\n",
    "        split_data.append(copy.deepcopy(dataset))\n",
    "        to_add = [1 if label[i] == 1 else 0 for label in labels]\n",
    "        split_labels.append(to_add)\n",
    "        \n",
    "    return split_data, split_labels \n",
    "\n",
    "split_train_images, split_labels = split_data(train_data, train_labels, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140702, 784)\n",
      "(152752, 784)\n",
      "(138144, 784)\n",
      "(128864, 784)\n",
      "(148704, 784)\n",
      "(147910, 784)\n",
      "(154696, 784)\n",
      "(149252, 784)\n",
      "(150130, 784)\n",
      "(153274, 784)\n",
      "(153050, 784)\n",
      "(154340, 784)\n",
      "(152102, 784)\n"
     ]
    }
   ],
   "source": [
    "def shuffle_together(data, labels):\n",
    "    p = np.random.permutation(len(data))\n",
    "    return data[p], labels[p]\n",
    "\n",
    "# def subsample_class(data, labels, majority_class):\n",
    "#     all_indices = np.array(get_equal_indicies(labels, majority_class))\n",
    "\n",
    "#     excluded_indices = [all_indices[(random.randint(0,len(all_indices)-1))] for i in range(len(all_indices))]\n",
    "#     excluded_indicies = set(excluded_indices)\n",
    "#     chosen_indicies = np.array([i for i in range(np.shape(data)[0]) if i not in excluded_indicies])\n",
    "\n",
    "#     return data[chosen_indicies], labels[chosen_indicies]\n",
    "\n",
    "# def oversample_class(data, labels, minority_class, copy_num, class_num):\n",
    "#     all_indices = [i for i in get_equal_indicies(labels, minority_class) if list(labels[i]).count(1) == 1]\n",
    "#     data_to_add = np.zeros((copy_num, 28, 28))\n",
    "#     labels_to_add = np.zeros((copy_num, class_num), dtype=int)\n",
    "\n",
    "#     for i in range(copy_num):\n",
    "#         rnd_index = random.randint(0,len(all_indices)-1)\n",
    "#         data_to_add[i] = data[all_indices[rnd_index]].copy()\n",
    "#         labels_to_add[i] = labels[all_indices[rnd_index]].copy()\n",
    "\n",
    "#     # labels_to_add = np.reshape(labels_to_add, (np.shape(labels_to_add)[0]))\n",
    "#     data = np.append(data, data_to_add, axis=0)\n",
    "#     labels = np.append(labels, labels_to_add, axis=0)\n",
    "\n",
    "#     return data, labels\n",
    "\n",
    "# def oversample_binary(data, labels, copy_num):\n",
    "#     chosen_indicies = [i for i in range(np.shape(data)[0]) if labels[i] == 1]\n",
    "#     data_to_add = np.zeros((copy_num, 28, 28))\n",
    "\n",
    "#     for i in range(copy_num):\n",
    "#         rnd_index = random.randint(0,len(chosen_indicies)-1)\n",
    "#         data_to_add[i] = data[chosen_indicies[rnd_index]].copy()\n",
    "    \n",
    "#     data = np.append(data, data_to_add, axis=0)\n",
    "#     labels = np.append(labels, np.ones(copy_num))\n",
    "\n",
    "#     data, labels = shuffle_together(data, labels)\n",
    "#     return data, labels\n",
    "\n",
    "def oversample_dataset(split_dataset, split_labels):\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    for class_index in range(len(split_dataset)):\n",
    "        split_labels[class_index] = np.array(split_labels[class_index])\n",
    "        split_dataset[class_index] = split_dataset[class_index].reshape((78324,784))\n",
    "        split_dataset[class_index], split_labels[class_index] = ros.fit_resample(split_dataset[class_index], split_labels[class_index])\n",
    "        print(np.shape(split_dataset[class_index]))\n",
    "        length = np.shape(split_dataset[class_index])[0]\n",
    "        split_dataset[class_index] = split_dataset[class_index].reshape((np.shape(split_dataset[class_index])[0],28,28))\n",
    "\n",
    "\n",
    "    return split_dataset, split_labels\n",
    "\n",
    "\n",
    "split_train_images, split_labels = oversample_dataset(split_train_images, split_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def loss(target, output):\n",
    "#     weights = np.array([0.4999014531185609, 1.023027280727486, 0.43079488604780436, 0.2869071613672823, 1.0034527406128615, 0.9122715233953503, 1.0209309133489461, 1.077802673259677, 1.222986893437952, 1.181302396477263, 1.1077582784086397, 0.8634563010646199, 0.8767519326252279, 0.6890930646117368])\n",
    "#     target = tf.convert_to_tensor(target)\n",
    "#     output = tf.convert_to_tensor(output)\n",
    "#     target.shape.assert_is_compatible_with(output.shape)\n",
    "#     weights = tf.reshape(tf.convert_to_tensor(weights, dtype=target.dtype), (1,-1))\n",
    "\n",
    "#     # Adjust the predictions so that the probability of\n",
    "#     # each class for every sample adds up to 1\n",
    "#     # This is needed to ensure that the cross entropy is\n",
    "#     # computed correctly.\n",
    "#     output = output / tf.reduce_sum(output, -1, True)\n",
    "\n",
    "#     # Compute cross entropy from probabilities.\n",
    "#     epsilon_ = tf.constant(tf.keras.backend.epsilon(), output.dtype.base_dtype)\n",
    "#     output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n",
    "#     return -tf.reduce_sum(weights * target * tf.math.log(output), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_models = [0]*class_num\n",
    "\n",
    "for i in range(class_num):\n",
    "    cnn_models[i] = Sequential()\n",
    "    cnn_models[i].add(Conv2D(filters=10, kernel_size=(3, 3), activation='relu', input_shape= (28, 28, 1)))\n",
    "    cnn_models[i].add(MaxPooling2D(2, 2))\n",
    "    cnn_models[i].add(Conv2D(filters=10, kernel_size=(3, 3), activation='relu', input_shape= (28, 28, 1)))\n",
    "    cnn_models[i].add(MaxPooling2D(2, 2))\n",
    "    cnn_models[i].add(Flatten())\n",
    "    cnn_models[i].add(Dense(10, activation='relu'))\n",
    "    cnn_models[i].add(Dense(1, activation='sigmoid'))\n",
    "    cnn_models[i].compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=['AUC', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2199/2199 [==============================] - 16s 6ms/step - loss: 0.6929 - auc: 0.6386 - accuracy: 0.6012\n",
      "Epoch 2/5\n",
      "2199/2199 [==============================] - 13s 6ms/step - loss: 0.6356 - auc: 0.6895 - accuracy: 0.6393\n",
      "Epoch 3/5\n",
      "2199/2199 [==============================] - 13s 6ms/step - loss: 0.6278 - auc: 0.7006 - accuracy: 0.6468\n",
      "Epoch 4/5\n",
      "2199/2199 [==============================] - 13s 6ms/step - loss: 0.6215 - auc: 0.7089 - accuracy: 0.6533\n",
      "Epoch 5/5\n",
      "2199/2199 [==============================] - 13s 6ms/step - loss: 0.6168 - auc: 0.7155 - accuracy: 0.6579\n",
      "Feature:  1\n",
      "Epoch 1/5\n",
      "2387/2387 [==============================] - 17s 6ms/step - loss: 0.5705 - auc: 0.7808 - accuracy: 0.7070\n",
      "Epoch 2/5\n",
      "2387/2387 [==============================] - 14s 6ms/step - loss: 0.4585 - auc: 0.8590 - accuracy: 0.7753\n",
      "Epoch 3/5\n",
      "2387/2387 [==============================] - 15s 6ms/step - loss: 0.4094 - auc: 0.8887 - accuracy: 0.8067\n",
      "Epoch 4/5\n",
      "2387/2387 [==============================] - 14s 6ms/step - loss: 0.3771 - auc: 0.9052 - accuracy: 0.8257\n",
      "Epoch 5/5\n",
      "2387/2387 [==============================] - 14s 6ms/step - loss: 0.3491 - auc: 0.9179 - accuracy: 0.8421\n",
      "Feature:  2\n",
      "Epoch 1/5\n",
      "2159/2159 [==============================] - 14s 6ms/step - loss: 0.6403 - auc: 0.6996 - accuracy: 0.6437\n",
      "Epoch 2/5\n",
      "2159/2159 [==============================] - 14s 7ms/step - loss: 0.5846 - auc: 0.7582 - accuracy: 0.6938\n",
      "Epoch 3/5\n",
      "2159/2159 [==============================] - 13s 6ms/step - loss: 0.5669 - auc: 0.7769 - accuracy: 0.7118\n",
      "Epoch 4/5\n",
      "2159/2159 [==============================] - 14s 7ms/step - loss: 0.5595 - auc: 0.7841 - accuracy: 0.7186\n",
      "Epoch 5/5\n",
      "2159/2159 [==============================] - 15s 7ms/step - loss: 0.5531 - auc: 0.7901 - accuracy: 0.7234\n",
      "Feature:  3\n",
      "Epoch 1/5\n",
      "2014/2014 [==============================] - 14s 6ms/step - loss: 0.7367 - auc: 0.5941 - accuracy: 0.5759\n",
      "Epoch 2/5\n",
      "2014/2014 [==============================] - 12s 6ms/step - loss: 0.6619 - auc: 0.6432 - accuracy: 0.6059\n",
      "Epoch 3/5\n",
      "2014/2014 [==============================] - 13s 6ms/step - loss: 0.6550 - auc: 0.6561 - accuracy: 0.6158\n",
      "Epoch 4/5\n",
      "2014/2014 [==============================] - 12s 6ms/step - loss: 0.6512 - auc: 0.6630 - accuracy: 0.6199\n",
      "Epoch 5/5\n",
      "2014/2014 [==============================] - 14s 7ms/step - loss: 0.6489 - auc: 0.6673 - accuracy: 0.6221\n",
      "Feature:  4\n",
      "Epoch 1/5\n",
      "2324/2324 [==============================] - 16s 7ms/step - loss: 0.6931 - auc: 0.5001 - accuracy: 0.4995\n",
      "Epoch 2/5\n",
      "2324/2324 [==============================] - 16s 7ms/step - loss: 0.6918 - auc: 0.5034 - accuracy: 0.5037\n",
      "Epoch 3/5\n",
      "2324/2324 [==============================] - 14s 6ms/step - loss: 0.6905 - auc: 0.5074 - accuracy: 0.5055\n",
      "Epoch 4/5\n",
      "2324/2324 [==============================] - 14s 6ms/step - loss: 0.6704 - auc: 0.6079 - accuracy: 0.5688\n",
      "Epoch 5/5\n",
      "2324/2324 [==============================] - 14s 6ms/step - loss: 0.6224 - auc: 0.7072 - accuracy: 0.6546\n",
      "Feature:  5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train_data = train_data.astype('float32')\n",
    "for i in range(class_num):\n",
    "    print(\"Feature:\", i)\n",
    "    cnn_models[i].fit(split_train_images[i], split_labels[i], batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4162689e-12 1.5216710e-08 4.5782864e-10 1.4946076e-10 3.0666295e-05\n",
      " 9.9996936e-01 7.4342017e-09]\n",
      "[0.01505083 0.0504419  0.03308822 0.00276871 0.6952178  0.20125857\n",
      " 0.00217384]\n",
      "[4.7845774e-06 2.3109933e-04 2.5010526e-05 1.0316935e-06 3.2762840e-02\n",
      " 9.6694505e-01 3.0211440e-05]\n",
      "[0.09250011 0.22203088 0.07604286 0.05883425 0.23826015 0.23454644\n",
      " 0.07778532]\n",
      "[2.7494119e-03 1.6411096e-02 1.2630174e-01 5.5240397e-04 6.2206441e-01\n",
      " 2.3104410e-01 8.7681267e-04]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
