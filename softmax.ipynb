{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 07:24:10.995904: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 07:24:11.578016: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 07:24:11.578204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 07:24:11.656922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 07:24:11.858370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 07:24:13.633147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, AveragePooling2D, Input\n",
    "from keras import layers\n",
    "from keras import Sequential, optimizers, layers\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow.random as random_tf\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from npz file,\n",
    ".npz is used to save numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11959, 28, 28, 3)\n",
      "(1712, 28, 28, 3)\n",
      "(3421, 28, 28, 3)\n",
      "(11959, 1)\n",
      "(1712, 1)\n",
      "(3421, 1)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"bloodmnist.npz\")\n",
    "train_images = data[\"train_images\"]\n",
    "print(np.shape(data[\"train_images\"]))\n",
    "val_images = data[\"val_images\"]\n",
    "print(np.shape(data[\"val_images\"]))\n",
    "test_images = data[\"test_images\"]\n",
    "print(np.shape(data[\"test_images\"]))\n",
    "train_labels = data[\"train_labels\"]\n",
    "print(np.shape(data[\"train_labels\"]))\n",
    "val_labels = data[\"val_labels\"]\n",
    "print(np.shape(data[\"val_labels\"]))\n",
    "test_labels = data[\"test_labels\"]\n",
    "print(np.shape(data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        self.length = np.shape(images)[0]\n",
    "        self.width = np.shape(images)[1]\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    def update_counts(self):\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            self.counts.append(len(np.where(self.labels == i)[0]))\n",
    "        \n",
    "        self.proportions = [count/self.length for count in self.counts]\n",
    "\n",
    "    def oversample(self):\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        self.images= self.images.reshape((self.length, self.width*self.width*3))\n",
    "        self.images, self.labels = ros.fit_resample(self.images, self.labels)\n",
    "        self.length = self.images.shape[0]\n",
    "        print(self.length)\n",
    "        print(np.shape(self.images))\n",
    "        print(self.labels.shape)\n",
    "        self.images = self.images.reshape((self.length, self.width, self.width, 3))\n",
    "        # print(self.images[0])\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        one_hot_labels = np.array([np.zeros(self.class_num) for i in range(self.length)])\n",
    "        for i in range(self.length):\n",
    "            one_hot_labels[i][self.labels[i]] = 1\n",
    "        return one_hot_labels\n",
    "    \n",
    "    # def apply_CNN(self, model):\n",
    "\n",
    "    def shuffle(self):\n",
    "        p = np.random.permutation(self.length)\n",
    "        self.images, self.labels, self.one_hot_labels = self.images[p], self.labels[p], self.one_hot_labels[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise classes for training, validation and testing validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[852, 2181, 1085, 2026, 849, 993, 2330, 1643] [0.07124341500125428, 0.182373108119408, 0.09072664938540012, 0.16941215820720795, 0.07099255790617945, 0.08303369846977172, 0.1948323438414583, 0.1373860690693202]\n",
      "[122, 312, 155, 290, 122, 143, 333, 235] [0.07126168224299065, 0.1822429906542056, 0.0905373831775701, 0.169392523364486, 0.07126168224299065, 0.08352803738317757, 0.19450934579439252, 0.13726635514018692]\n",
      "[244, 624, 311, 579, 243, 284, 666, 470] [0.07132417421806489, 0.1824028061970184, 0.09090909090909091, 0.16924875767319497, 0.0710318620286466, 0.08301666179479684, 0.19467991815258695, 0.1373867290266004]\n"
     ]
    }
   ],
   "source": [
    "# Initialise Class for training, validation, test\n",
    "train_dataset = ImageDataset(train_images, train_labels)\n",
    "val_dataset = ImageDataset(val_images, val_labels)\n",
    "test_dataset = ImageDataset(test_images, test_labels)\n",
    "\n",
    "# print counts and proportions to see if data needs to be balanced\n",
    "print(train_dataset.counts, train_dataset.proportions)\n",
    "print(val_dataset.counts, val_dataset.proportions)\n",
    "print(test_dataset.counts, test_dataset.proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset found to be imbalanced, oversample training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18640\n",
      "(18640, 2352)\n",
      "(18640,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset.oversample()\n",
    "# print(train_dataset.counts)\n",
    "# print(np.shape(train_dataset.images))\n",
    "# print(np.shape(train_dataset.one_hot_labels))\n",
    "# one hot encode labels\n",
    "\n",
    "# def one_hot_encode(label):\n",
    "#     return utils.to_categorical(label, num_classes=train_dataset.class_num)\n",
    "# one_hot_train = np.array([utils.to_categorical(label, num_classes=train_dataset.class_num) for label in train_dataset.labels])\n",
    "# print(np.shape(one_hot_train))\n",
    "# one_hot_train = one_hot_train.reshape(np.shape(one_hot_train)[0], train_dataset.class_num)\n",
    "# print(np.shape(one_hot_train))\n",
    "\n",
    "train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Data Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preprocessing(model):\n",
    "    # model.add(layers.RandomBrightness(factor=0.2))\n",
    "    model.add(layers.RandomFlip(mode=\"horizontal_and_vertical\"))\n",
    "    model.add(layers.RandomZoom(height_factor=0.2))\n",
    "    # model.add(layers.RandomRotation(factor=0.2))\n",
    "    model.add(layers.RandomContrast(factor=0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Input(shape = (28, 28, 3)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape = (28, 28, 3), strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', strides=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "optimizer = optimizers.SGD(0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['AUC', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2330/2330 [==============================] - 14s 5ms/step - loss: 1.6840 - auc: 0.7743 - categorical_accuracy: 0.3643\n",
      "Epoch 2/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.9985 - auc: 0.9287 - categorical_accuracy: 0.6172\n",
      "Epoch 3/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.8574 - auc: 0.9470 - categorical_accuracy: 0.6691\n",
      "Epoch 4/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.7748 - auc: 0.9564 - categorical_accuracy: 0.7037\n",
      "Epoch 5/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.7199 - auc: 0.9622 - categorical_accuracy: 0.7287\n",
      "Epoch 6/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.6665 - auc: 0.9676 - categorical_accuracy: 0.7518\n",
      "Epoch 7/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.6217 - auc: 0.9719 - categorical_accuracy: 0.7638\n",
      "Epoch 8/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.5858 - auc: 0.9748 - categorical_accuracy: 0.7805\n",
      "Epoch 9/20\n",
      "2330/2330 [==============================] - 12s 5ms/step - loss: 0.5450 - auc: 0.9780 - categorical_accuracy: 0.7963\n",
      "Epoch 10/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.5150 - auc: 0.9803 - categorical_accuracy: 0.8073\n",
      "Epoch 11/20\n",
      "2330/2330 [==============================] - 13s 6ms/step - loss: 0.4808 - auc: 0.9826 - categorical_accuracy: 0.8217\n",
      "Epoch 12/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.4561 - auc: 0.9844 - categorical_accuracy: 0.8280\n",
      "Epoch 13/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.4265 - auc: 0.9861 - categorical_accuracy: 0.8416\n",
      "Epoch 14/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.4007 - auc: 0.9876 - categorical_accuracy: 0.8507\n",
      "Epoch 15/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.3787 - auc: 0.9888 - categorical_accuracy: 0.8604\n",
      "Epoch 16/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.3551 - auc: 0.9900 - categorical_accuracy: 0.8675\n",
      "Epoch 17/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.3361 - auc: 0.9909 - categorical_accuracy: 0.8781\n",
      "Epoch 18/20\n",
      "2330/2330 [==============================] - 15s 7ms/step - loss: 0.3192 - auc: 0.9917 - categorical_accuracy: 0.8823\n",
      "Epoch 19/20\n",
      "2330/2330 [==============================] - 13s 6ms/step - loss: 0.3034 - auc: 0.9926 - categorical_accuracy: 0.8896\n",
      "Epoch 20/20\n",
      "2330/2330 [==============================] - 13s 5ms/step - loss: 0.2848 - auc: 0.9932 - categorical_accuracy: 0.8963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb81d0b9f30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset.images, train_dataset.one_hot_labels, batch_size=8, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 2s 5ms/step - loss: 0.3610 - auc: 0.9886 - categorical_accuracy: 0.8709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-dv/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(val_dataset.images, val_dataset.one_hot_labels)\n",
    "model.save(\"./CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVRDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.images = images/255\n",
    "        self.labels = labels\n",
    "        self.splits = self.create_splits()\n",
    "\n",
    "    def create_splits(self):\n",
    "        splits = []\n",
    "        \n",
    "        for i in range(self.class_num):\n",
    "            binary_labels = np.array([1 if label == i else 0 for label in self.labels])\n",
    "            splits.append(ImageDataset(self.images.copy(), binary_labels))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def oversample_dataset(self):\n",
    "        for split in self.splits:\n",
    "            split.oversample()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_' is not defined"
     ]
    }
   ],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # define dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# # define model\n",
    "# model = LogisticRegression(multi_class='ovr')\n",
    "# # fit model\n",
    "# model.fit(X, y)\n",
    "# # make predictions\n",
    "# yhat = model.predict(X)\n",
    "\n",
    "# trainOVR.oversample_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
