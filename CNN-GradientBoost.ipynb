{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Input\n",
    "from keras import Sequential, optimizers, layers, Model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import keras.backend as K\n",
    "import keras_tuner as kt\n",
    "import random\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset from https://zenodo.org/records/10519652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset downloaded from https://zenodo.org/records/10519652\n",
    "data = np.load(\"bloodmnist.npz\")\n",
    "train_images = data[\"train_images\"]\n",
    "print(np.shape(data[\"train_images\"]))\n",
    "val_images = data[\"val_images\"]\n",
    "print(np.shape(data[\"val_images\"]))\n",
    "test_images = data[\"test_images\"]\n",
    "print(np.shape(data[\"test_images\"]))\n",
    "train_labels = data[\"train_labels\"]\n",
    "print(np.shape(data[\"train_labels\"]))\n",
    "val_labels = data[\"val_labels\"]\n",
    "print(np.shape(data[\"val_labels\"]))\n",
    "test_labels = data[\"test_labels\"]\n",
    "print(np.shape(data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset class for training, validation and testing\n",
    "class ImageDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.labels = labels\n",
    "        self.class_num = len(np.unique(labels))\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        self.length = np.shape(images)[0]\n",
    "        self.width = np.shape(images)[1]\n",
    "\n",
    "        # one hot encode the labels to show non ordering of data\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "\n",
    "        # get proportions of each label in dataset\n",
    "        self.update_counts()\n",
    "\n",
    "        # normalise RGB values between 0 and 1\n",
    "        self.images = images/255\n",
    "\n",
    "    def update_counts(self):\n",
    "        self.counts = []\n",
    "        self.proportions = []\n",
    "        \n",
    "        # get number of occurences of each class\n",
    "        for i in range(self.class_num):\n",
    "            self.counts.append(len(np.where(self.labels == i)[0]))\n",
    "        \n",
    "        self.proportions = [count/self.length for count in self.counts]\n",
    "\n",
    "    # function to oversample dataset so that all the classes have equal counts\n",
    "    def oversample(self):\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "        # flatten images for oversampling\n",
    "        self.images= self.images.reshape((self.length, self.width*self.width*3))\n",
    "        self.images, self.labels = ros.fit_resample(self.images, self.labels)\n",
    "        self.length = self.images.shape[0]\n",
    "\n",
    "        # reshape back to images\n",
    "        self.images = self.images.reshape((self.length, self.width, self.width, 3))\n",
    "        self.one_hot_labels = self.one_hot_encode()\n",
    "        self.update_counts()\n",
    "\n",
    "    # function to one hot encode labels given integer labels\n",
    "    def one_hot_encode(self):\n",
    "        one_hot_labels = np.array([np.zeros(self.class_num) for i in range(self.length)])\n",
    "        for i in range(self.length):\n",
    "            one_hot_labels[i][self.labels[i]] = 1\n",
    "\n",
    "        return one_hot_labels\n",
    "    \n",
    "    # shuffles dataset \n",
    "    def shuffle(self):\n",
    "        # generate random permutation for shuffled indices of images and labels\n",
    "        p = np.random.permutation(self.length)\n",
    "        self.images, self.labels, self.one_hot_labels = self.images[p], self.labels[p], self.one_hot_labels[p]\n",
    "\n",
    "    # augment to reduce overfitting using operation\n",
    "    def augment(self, operation, factor):\n",
    "        # flip some images and add to dataset\n",
    "        new_images_num = int(factor*self.length)\n",
    "        p = np.random.permutation(self.length)[:new_images_num]\n",
    "        new_images = operation(self.images[p])\n",
    "        new_labels = self.labels[p]\n",
    "        self.images = np.append(self.images, new_images, axis = 0)\n",
    "        self.labels = np.append(self.labels, new_labels, axis = 0)\n",
    "\n",
    "    # augment all images such that 50% of dataset is augmented\n",
    "    def augment_images(self):\n",
    "        # define augmentations using keras augmentation layers\n",
    "        flip = layers.RandomFlip(mode=\"horizontal_and_vertical\")\n",
    "        zoom = layers.RandomZoom(height_factor=0.5)\n",
    "        translation = layers.RandomTranslation(height_factor=0.3, width_factor=0.3)\n",
    "        rotate = layers.RandomRotation(factor=0.5)\n",
    "\n",
    "        # flip some images\n",
    "        self.augment(flip, 0.1)\n",
    "\n",
    "        # zoom in on some images\n",
    "        self.augment(zoom, 0.2)\n",
    "\n",
    "        # translate some images\n",
    "        self.augment(translation, 0.4)\n",
    "\n",
    "        # rotate some images\n",
    "        self.augment(rotate, 0.2)\n",
    "\n",
    "    # use CNN model to get features from images\n",
    "    def get_features(self, model):\n",
    "        self.images = model.predict(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot counts of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the counts of each class\n",
    "def plot_label_counts(dataset):\n",
    "    x = np.array(list(range(8)))\n",
    "    y = np.array(dataset.counts)\n",
    "    plt.bar(x, y)\n",
    "    plt.savefig(\"label_counts.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare training dataset\n",
    "def prepare_dataset(dataset):\n",
    "    dataset.oversample()\n",
    "    dataset.augment_images()\n",
    "    dataset.shuffle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class to tune hyperparameters of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN class to tune hyperparameters\n",
    "class CNN(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        # define hyperpamaters search space for tuning\n",
    "        filters_1 = hp.Int('filters_1', min_value=16, max_value=512, step=32)\n",
    "        filters_2 = hp.Int('filters_2', min_value=16, max_value=512, step=32)\n",
    "        filters_3 = hp.Int('filters_3', min_value=16, max_value=512, step=32)\n",
    "        dense_1_size = hp.Int('size_1', min_value=10, max_value=510, step=50)\n",
    "        learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape = (28, 28, 3)))\n",
    "        model.add(Conv2D(filters=filters_1, kernel_size=(3, 3), activation='relu', input_shape = (28, 28, 3), strides=1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        model.add(Conv2D(filters=filters_2, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        model.add(Conv2D(filters = filters_3, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(.1))\n",
    "        model.add(Dense(dense_1_size, activation='relu'))\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['AUC', 'categorical_accuracy', get_f1, get_recall, get_precision])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=8,\n",
    "            verbose=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get optimal CNN hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams(train_dataset, val_dataset):\n",
    "    tuner = kt.RandomSearch(\n",
    "        CNN(),\n",
    "        objective='val_categorical_accuracy',\n",
    "        directory='optimal_parameters',\n",
    "        project_name='train_CNN'\n",
    "    )\n",
    "\n",
    "    tuner.search(train_dataset.images, train_dataset.one_hot_labels, epochs=11, validation_data=(val_dataset.images, val_dataset.one_hot_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metric functions (from old keras source code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from old Keras source code\n",
    "def get_f1(y_true, y_pred): \n",
    "    precision = get_precision(y_true, y_pred)\n",
    "    recall = get_recall(y_true, y_pred)\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "# function to get precision using Keras backend\n",
    "def get_precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# function to get recall using Keras backend\n",
    "def get_recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make CNN model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make CNN model with tuned hyperparameters\n",
    "def CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (28, 28, 3)))\n",
    "    model.add(Conv2D(filters=48, kernel_size=(3, 3), activation='relu', input_shape = (28, 28, 3), strides=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(Conv2D(filters=240, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(Conv2D(filters = 208, kernel_size=(3, 3), activation='relu', strides=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.1))\n",
    "    model.add(Dense(90, activation='relu'))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['AUC', 'categorical_accuracy', get_f1, get_recall, get_precision])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model and get learning history of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model and get history of train vs val\n",
    "def train_model(model, train_dataset, val_dataset):\n",
    "    history = model.fit(train_dataset.images, train_dataset.one_hot_labels, validation_data=(val_dataset.images, val_dataset.one_hot_labels), batch_size=8, epochs=10, verbose=0)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot learning history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over time to verify that model has converged and not overfitted\n",
    "def plot_learning_history(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.title(f'model {metric}')\n",
    "    plt.ylabel(f'{metric}')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(f\"{metric}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get confusion matrix of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show confusion matrix on test dataset\n",
    "def cnn_confusion_matrix(test_dataset, model):\n",
    "    confusion_matrix = metrics.confusion_matrix(np.argmax(model.predict(test_dataset.images), axis=1), test_dataset.labels)\n",
    "    confusion_matrix_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=list(range(8)))\n",
    "    confusion_matrix_display.plot()\n",
    "    plt.savefig(\"confusing_matrix.png\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model for use by Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CNN layers of trained CNN model\n",
    "def get_model(model):\n",
    "    model_use = Model(\n",
    "        inputs = model.input,\n",
    "        outputs = model.layers[-3].output\n",
    "    )\n",
    "\n",
    "    return model_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine dataset to show counts of each class and perform my own splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dataset of iamges and labels\n",
    "whole_images = np.append(train_images, val_images, axis=0)\n",
    "whole_images = np.append(whole_images, test_images, axis=0)\n",
    "\n",
    "whole_labels = np.append(train_labels, val_labels, axis=0)\n",
    "whole_labels = np.append(whole_labels, test_labels, axis=0)\n",
    "\n",
    "whole_dataset = ImageDataset(whole_images, whole_labels)\n",
    "plot_label_counts(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into testing dataset and training dataset with ratio 2:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test and train with ratio 80% to 20%\n",
    "cv_images, test_images, cv_labels, test_labels = train_test_split(whole_images, whole_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform K cross validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None, shuffle=False)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(cv_images)):\n",
    "\n",
    "    # for each split, make training and validation dataset\n",
    "    train_images = cv_images[train_index]\n",
    "    val_images = cv_images[val_index]\n",
    "    train_labels = cv_labels[train_index]\n",
    "    val_labels = cv_labels[val_index]\n",
    "    train_dataset = ImageDataset(train_images, train_labels)\n",
    "    val_dataset = ImageDataset(val_images, val_labels)\n",
    "\n",
    "    # oversample, train and augment dataset\n",
    "    prepare_dataset(train_dataset)\n",
    "\n",
    "    # get CNN model\n",
    "    model = CNN()\n",
    "\n",
    "    # train CNN model\n",
    "    history = train_model(model, train_dataset, val_dataset)\n",
    "\n",
    "    # evalute CNN model on validation dataset\n",
    "    results = model.evaluate(val_dataset.images, val_dataset.one_hot_labels)\n",
    "\n",
    "    # plot learning history of loss\n",
    "    plot_learning_history(history, \"loss\")\n",
    "\n",
    "    # get CNN model to use for Gradient Boosting\n",
    "    cnn_model = get_model(model)\n",
    "\n",
    "    # use CNN model to get features\n",
    "    train_dataset.get_features(cnn_model)\n",
    "    val_dataset.get_features(cnn_model)\n",
    "\n",
    "    grad_boost = HistGradientBoostingClassifier(max_depth=4, max_iter=200, learning_rate=0.09, random_state=0)\n",
    "    grad_boost.fit(train_dataset.images, train_dataset.labels)\n",
    "    trainhat = grad_boost.predict(train_dataset.images)\n",
    "    valhat = grad_boost.predict(val_dataset.images)\n",
    "\n",
    "\n",
    "    acc = metrics.accuracy_score(val_dataset.labels, valhat)\n",
    "    val_f1_score = metrics.f1_score(val_dataset.labels, valhat, average=\"macro\")\n",
    "    val_precision = metrics.precision_score(val_dataset.labels, valhat, average = \"macro\")\n",
    "    val_recall = metrics.recall_score(val_dataset.labels, valhat, average = \"macro\")\n",
    "\n",
    "    print(\"Softmax test results:\", results)\n",
    "    print(\"Accuracy for gradient boosting:\", acc)\n",
    "    print(\"F1 Score for gradient boosting:\", val_f1_score)\n",
    "    print(\"Precision for gradient boosting:\", val_precision)\n",
    "    print(\"Recall for gradient boosting:\", val_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine validation and training and test both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CNN model\n",
    "model = CNN()\n",
    "\n",
    "# for last training, combine training and validation into one dataset\n",
    "final_training = ImageDataset(cv_images, cv_labels)\n",
    "\n",
    "# make testing dataset\n",
    "test_dataset = ImageDataset(test_images, test_labels)\n",
    "\n",
    "# train CNN\n",
    "history = train_model(model, final_training, test_dataset)\n",
    "cnn_model = get_model(model)\n",
    "final_training.get_features(cnn_model)\n",
    "grad_boost = HistGradientBoostingClassifier(max_depth=4, max_iter=200, learning_rate=0.09, random_state=0)\n",
    "grad_boost.fit(final_training.images, final_training.labels)\n",
    "\n",
    "# get confusion matrix of CNN model\n",
    "print(\"Testing: \")\n",
    "results = model.evaluate(test_dataset.images, test_dataset.one_hot_labels)\n",
    "y_pred = np.argmax(model.predict(test_dataset.images), axis=1)\n",
    "cnn_confusion_matrix(test_dataset, model)\n",
    "test_dataset.get_features(cnn_model)\n",
    "testhat = grad_boost.predict(test_dataset.images)\n",
    "# testhat = np.argmax(testhat, axis=1)\n",
    "print(metrics.classification_report(test_dataset.labels, y_pred))\n",
    "print(metrics.classification_report(test_dataset.labels, testhat))\n",
    "print(\"Softmax test results:\", results)\n",
    "# get confusion matrix of random boost\n",
    "grad_boost_confusion = metrics.confusion_matrix(test_dataset.labels, testhat, labels = range(8))\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=grad_boost_confusion, display_labels = range(8))\n",
    "disp.plot()\n",
    "plt.savefig(\"grad_boost_confusion_matrix\")\n",
    "plt.show()\n",
    "\n",
    "# print metrics of gradient boost\n",
    "acc = metrics.accuracy_score(test_dataset.labels, testhat)\n",
    "test_f1_score = metrics.f1_score(test_dataset.labels, testhat, average=\"macro\")\n",
    "test_precision = metrics.precision_score(test_dataset.labels, testhat, average = \"macro\")\n",
    "test_recall = metrics.recall_score(test_dataset.labels, testhat, average = \"macro\")\n",
    "print(\"Accuracy for gradient boosting:\", acc)\n",
    "print(\"F1 Score for gradient boosting:\", test_f1_score)\n",
    "print(\"Precision for gradient boosting:\", test_precision)\n",
    "print(\"Recall for gradient boosting:\", test_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
